{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd00cf3f-24a9-4d61-97dd-01f75259447f",
   "metadata": {},
   "source": [
    "# Attention Mechanism "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be30a13-7f8c-4551-864b-230d2855c1a6",
   "metadata": {},
   "source": [
    "- Seq2Seq의 입력 Sequence를 Encoder에 의해 하나의 Context Vector로 변환하고, Decoder가 이 Vector를 이용해 출력 Sequence 구성\n",
    "- 단점 :\n",
    "  - 1. Context Vector의 고정된 크기(Columns -> Matrix의 Row x Col)로 인해 정보 손실\n",
    "  - 2. 긴 Sequence의 경우, RNN 계열 모델들의 장기기억소실/기울기소실 문제들이 발생\n",
    "\n",
    "- Attention Mechanism : Decoder에서 출력 단어를 생성할 때, 단어를 예측하느 매 시점(Time Step)마다 Encoder의 전체 입력문장의 정보를 반영하여 출력\n",
    "- 해당 시점에서 예측해야할 단어와 연관있는 입력단어의 부분을 좀 더 집중(Attention)하여 출력\n",
    "- 작동 :\n",
    "  - 모든 단어 쌍에 대한 key Value 형태의 유사도 분석을 수행\n",
    "  - 해당 유사도를 이용하여 Attention Score 계산\n",
    "  - Attention Score : 현재 Decoder의 특정 t 시점에서 단어를 예측하기 위해, Encoder의 각 시점의 값이 Decoder의 각 시점의 값과 얼마나 유사한지를 계산\n",
    "  - Attention Score를 이용해 분포추정 실시 -> 출력 단계엣 좀 더 유사한 단어 쌍이 생성되도록 유도\n",
    "\n",
    "![image1](https://i0.wp.com/blog.kakaocdn.net/dn/RtMGM/btrXyiWoa10/5zt5t6BgqEXrtsU13OpHdK/img.png?w=900&ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7576ce63-7f9f-4745-93f0-8d6c3033ef5d",
   "metadata": {},
   "source": [
    "# 트랜스포머 (Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe1a07-0018-4379-8b7b-4933505b72b9",
   "metadata": {},
   "source": [
    "- Attention Mechanism을 응용하여, 이전의 RNN, LSTM모델에서 Sequence 단위의 데이터를 순차적으로 처리하는 것과 달리, 전체 Sequence를 한번에 처리\n",
    "- **구성 요소**\n",
    "  - Self - Attention : 하나의 Sequence 내 각 위치의 토큰이 Sequence내 다른 위치 토큰과 얼마나 관련이 있는지를 계산\n",
    "      - 문장 내에 단어간의 등장 순서/ 관계 파악\n",
    "\n",
    "  - Multi-Head Attention : Self - Attention 층이 병렬로 반복적으로 처리되며 다양한 표현(Subspace)에서 정보를 추출\n",
    "      - 위치정보가 처리된 문장의 정보를 처리\n",
    "         \n",
    "  - Position Encoding : Attetion으로 구성된 모델에서 각 Sequence의 순서 정보를 유지하기 위해, 순서 정보를 가진 Position Encoding을 수행\n",
    " \n",
    "![image1](https://wikidocs.net/images/page/31379/transformer_attention_overview.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5742d38b-9313-49d5-abc9-cfb1b9d2e730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.9.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: absl-py in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (8.1.7)\n",
      "Collecting dm-tree (from tensorflow_datasets)\n",
      "  Downloading dm_tree-0.1.8-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting immutabledict (from tensorflow_datasets)\n",
      "  Downloading immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.26.4)\n",
      "Collecting promise (from tensorflow_datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (3.20.3)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (5.9.0)\n",
      "Requirement already satisfied: pyarrow in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (14.0.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (2.31.0)\n",
      "Collecting simple-parsing (from tensorflow_datasets)\n",
      "  Downloading simple_parsing-0.1.5-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
      "  Downloading tensorflow_metadata-1.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: termcolor in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (2.4.0)\n",
      "Requirement already satisfied: toml in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (4.65.0)\n",
      "Requirement already satisfied: wrapt in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.14.1)\n",
      "Collecting etils>=1.9.1 (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets)\n",
      "  Downloading etils-1.9.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2023.10.0)\n",
      "Collecting importlib_resources (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets)\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing_extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (4.9.0)\n",
      "Requirement already satisfied: zipp in c:\\programdata\\anaconda3\\lib\\site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->tensorflow_datasets) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Collecting docstring-parser~=0.15 (from simple-parsing->tensorflow_datasets)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow_datasets)\n",
      "  Downloading googleapis_common_protos-1.63.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf>=3.20 (from tensorflow_datasets)\n",
      "  Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Downloading tensorflow_datasets-4.9.6-py3-none-any.whl (5.1 MB)\n",
      "   ---------------------------------------- 0.0/5.1 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.8/5.1 MB 26.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.3/5.1 MB 29.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.7/5.1 MB 33.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.1/5.1 MB 35.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.1/5.1 MB 32.6 MB/s eta 0:00:00\n",
      "Downloading etils-1.9.2-py3-none-any.whl (161 kB)\n",
      "   ---------------------------------------- 0.0/161.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 161.5/161.5 kB 4.9 MB/s eta 0:00:00\n",
      "Downloading dm_tree-0.1.8-cp311-cp311-win_amd64.whl (101 kB)\n",
      "   ---------------------------------------- 0.0/101.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 101.3/101.3 kB ? eta 0:00:00\n",
      "Downloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading simple_parsing-0.1.5-py3-none-any.whl (113 kB)\n",
      "   ---------------------------------------- 0.0/113.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 113.6/113.6 kB ? eta 0:00:00\n",
      "Downloading tensorflow_metadata-1.15.0-py3-none-any.whl (28 kB)\n",
      "Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 413.4/413.4 kB 26.9 MB/s eta 0:00:00\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading googleapis_common_protos-1.63.1-py2.py3-none-any.whl (229 kB)\n",
      "   ---------------------------------------- 0.0/229.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 229.2/229.2 kB 14.6 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21545 sha256=3947ea6878ad8a183aa974d1752e2f8c056c07a8e08cc24dd5587a7deff882b7\n",
      "  Stored in directory: c:\\users\\userk\\appdata\\local\\pip\\cache\\wheels\\90\\74\\b1\\9b54c896b8d9409e9268329d4d45ede8a8040abe91c8879932\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, protobuf, promise, importlib_resources, immutabledict, etils, docstring-parser, simple-parsing, googleapis-common-protos, tensorflow-metadata, tensorflow_datasets\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "Successfully installed dm-tree-0.1.8 docstring-parser-0.16 etils-1.9.2 googleapis-common-protos-1.63.1 immutabledict-4.2.0 importlib_resources-6.4.0 promise-2.3 protobuf-4.25.3 simple-parsing-0.1.5 tensorflow-metadata-1.15.0 tensorflow_datasets-4.9.6\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3accbc94-249e-41df-b552-266ac43df69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bae4b455-166d-420f-93c2-367a381b7861",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/UserK/Desktop/Ranee/data/ML/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06e401eb-b7cc-45d6-8b4a-6950a4d386ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(path+'39_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e43f4e1e-e888-4b3d-aa80-7c1aabfaf11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수 문자에 대해 띄어쓰기를 수행 -> 토큰화 \n",
    "# ?!., 기호 앞뒤로 공백을 추가 -> 특수기호에 대한 공백 추가 후, strip 함수를 이용해 문장 앞뒤로 공백을 제거 \n",
    "questions = [re.sub(r\"([?.!,])\", r\" \\1 \", x).strip() for x in df1['Q']]\n",
    "answers   = [re.sub(r\"([?.!,])\", r\" \\1 \", x).strip() for x in df1['A']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5c37b01-7f4b-435e-a163-0dbbc1e293db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서브토크나이저 : 훈련 데이터에 없는 새로운 단어가 등장해도 그 단어를 구성하는 문자를 분해하여(서브 워드) 처리 할 수 있음 \n",
    "token_model = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8f480f9-b0a2-4aac-9def-a86089b4a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfds 라이브러리 오류 \n",
    "import pickle\n",
    "token_model = pickle.load(open('token.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b7cc827-83e6-4e28-bc14-8a2a03a5db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(token_model, open('token.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "debf8374-e543-4cc4-ad93-43838a5ace14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처리된 토큰 정보 \n",
    "# token_model.subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39858fe9-aa5f-44f9-ab90-69a79bb3e30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8178"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 만들어진 토큰 수 \n",
    "token_model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dac501c-49e6-43bf-8e7d-94c1db2b9387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOS 와 EOS 를 단어 사전에 추가 \n",
    "# SOS 와 EOS 가 들어갈 Index 부여 \n",
    "START_TOKEN, END_TOKEN = [token_model.vocab_size] , [token_model.vocab_size + 1] \n",
    "\n",
    "# SOS과 EOS 을 추가하기 위해 단어 사전의 공간을 늘림 \n",
    "VOCAB_SIZE = token_model.vocab_size + 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21499744-532a-466c-9a28-a8599e40030f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12시 땡 !\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7915, 4207, 3060, 41]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 앞서 구성한 서브토크나이저를 이용해 Text to Sequence \n",
    "print(questions[0])\n",
    "token_model.encode(questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8e8aff2-400d-44d1-9898-9cd987083b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'가스비 비싼데 감기 걸리겠어'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 숫자를 문자로 변환 가능 \n",
    "seq1 = [5766,611,3509,141,685,3747,849]\n",
    "token_model.decode(seq1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417ace71-1b96-40a9-bbc9-77aac366eb37",
   "metadata": {},
   "source": [
    "- 전체 데이터를 이용한 Text to Sequence와 Padding 작업을 수행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9950b6f3-aba7-4c83-b956-3bd71732be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "157b888e-2306-4248-b3df-6a2c0562f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 + 패딩 \n",
    "def token_pad(text):\n",
    "    text_list = [ ] # 토큰화 + 패딩 결과를 담을 변수를 선언 \n",
    "    for i in text :\n",
    "        sent1  = START_TOKEN + token_model.encode(i) + END_TOKEN # 문장을 Encoding 후, SOS 번호와 EOS 번호를 추가\n",
    "        text_list.append(sent1)\n",
    "    return pad_sequences(text_list, maxlen=40, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d5f2c1e-07f3-422f-9dd5-9ea00c003da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_out = token_pad(questions)\n",
    "answers_out   = token_pad(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0d4048-a3a6-4689-8da6-638d5102b2d2",
   "metadata": {},
   "source": [
    "- 앞서 처리된 질문/답변 행렬을 데이터 파일로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d043ec5d-a957-4d97-91f6-395913c63611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.data import Dataset\n",
    "from tensorflow.data.experimental import AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885e9ab1-065b-44f2-bc53-fd93a1ac5992",
   "metadata": {},
   "source": [
    "- 교사 강요 (Teacher Forcing) : 모델의 현재 출력을 다음 시점의 입력으로 사용하는 대신, 실제 목표 분장의 현재 시점의 데이터를 다음 시점의 입력으로 사용\n",
    "  - \"나는 고양이를 정말 좋아해\" 문장에서 \"고양이\" 단어가 나올 때, \"정말\"이라는 단어가 등장하게 학습이 되어야 하지만, \"강아지\"와 같은 전혀 다른 단어들이 출현 \n",
    "  - 학습 단계에서 이전 단어 다음의 단어를 정확하게 맞추지 못해도, 다음 단어의 입력을 강제로 정답을 알려주며 학습을 수행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f75f065-b4f8-47f6-bd89-91cf1e2db607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor flow의 Dataset를 이용해 배치사이즈로 데이터를 묶음 \n",
    "batch_size = 64 \n",
    "\n",
    "dataset = Dataset.from_tensor_slices( (\n",
    "    # 디코더의 입력 / 마지막 패딩 토큰을 제거 \n",
    "    {'inputs' : questions_out, 'dec_inputs' : answers_out[:, :-1]},\n",
    "    # 디코더의 출력에서 EOS 를 제거 \n",
    "    {'outputs' : answers_out[:, 1:]}\n",
    ") )\n",
    "\n",
    "# 데이터셋을 빠르게 로드하여 Epoch 수행 \n",
    "dataset1 = dataset.cache()\n",
    "# 데이터를 Batch Size에 맞게 묶음 \n",
    "dataset2 = dataset1.batch(batch_size)\n",
    "# 데이터의 로딩 시간을 줄이기 위해, 데이터를 미리 메모리에 로딩 \n",
    "dataset3 = dataset2.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "934e43f6-511f-4ef6-bdc9-c3e1d32f9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/chatbot_dataset'\n",
    "dataset3.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62a7d83-d97f-4137-bd12-23fab6f80000",
   "metadata": {},
   "source": [
    "# 포지셔널 인코딩 (Positional Encoding)\n",
    "\n",
    "- Transformer 모델의 입력층으로 사용되는 Layer\n",
    "- 기본적인 RNN 모델은 단어를 순차적으로 받아서 처리하기 때문에, 문장 내 단어의 순서가 처리되는 정보가 존재\n",
    "- 그러나 Transformer 모델은 데이터를 한번에 처리하기 때문에, 문장 내 단어의 순서 정보가 소실될 수 있음\n",
    "- ![image1](https://wikidocs.net/images/page/31379/transformer2.PNG)\n",
    "\n",
    "- Positional Encoding 기법으로 단어의 위치정보를 Matrix로 형태로 변환하여 학습을 수행\n",
    "- sin 함수와 cos 함수를 활용하여, 단어의 순서 정보를 Matrix 형태로 단어 Matrix와 함께 전달\n",
    "\n",
    "- ![image1](https://wikidocs.net/images/page/31379/transformer6_final.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433d42b7-e043-4129-9ae7-112c8a62b8af",
   "metadata": {},
   "source": [
    "# Transformer Attention \n",
    "\n",
    "- Attention Mechanism : 특정 Layer층에서 출력되는 매 시점 마다, 전체 문장을 다시 참조하여 출력 \n",
    "- 유사도를 계산하여, 가장 유사도가 높은 문장의 단어 벡터를 찾아 다음 Layer에 반영\n",
    "- 어텐션 메커니즘의 핵심 개념:\n",
    "    - 쿼리(Query): 현재 처리 중인 단어(또는 토큰)에 대한 표현\n",
    "    - 키(Key): 비교 대상이 되는 단어들의 표현\n",
    "    - 밸류(Value): 각 키와 쿼리 얻어진 유사도 값 \n",
    "\n",
    "- Transformer Model에서는 3파트의 Attention이 존재\n",
    "  - Multi-Head Attention (Encoder-Decoder Attention) : Encoder와 Decoder를 이어주는 Attention / Head를 통해 병렬로 처리\n",
    "  - Encoder Self Attention: 모델이 하나의 Seq 내에서 단어들 사이의 관계를 파악\n",
    "  - Masked Decoder Self Attention : 모델이 하나의 Seq 내에서 단어들 사이의 관계를 파악\n",
    " \n",
    "  - ![image2](https://wikidocs.net/images/page/31379/transformer_attention_overview.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a34d1d10-ad01-4df4-8bc8-e7feafa9d5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04185235-763e-4867-aeeb-5b48fdecb4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() \n",
    "\n",
    "# 하이퍼 파라미터 세팅 \n",
    "d_model = 256 # 트랜스 포머 내 모든 레이어의 Node 수 \n",
    "num_layer = 2 # 인코더와 디코더에 각각 있는 Layer 수 \n",
    "num_heads = 8 # 멀티-헤드 어텐션에서 사용되는 Head 수 \n",
    "dff = 512 # Feed Forward 신경망 내 Layer Node 수 \n",
    "drop_out = 0.1 # 10%의 랜덤 비율로 Layer Node를 비활성화 \n",
    "\n",
    "model = transformer_chatbot.transformer(\n",
    "    vocab_size= VOCAB_SIZE, # 단어 사전 수 \n",
    "    num_layers= num_layer,\n",
    "    dff = dff, d_model= d_model, num_heads=num_heads, dropout=drop_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cfca11c-b5d7-4a3e-bb06-311770bf1918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일을 위한 정확도 계산 함수를 구성 \n",
    "# 시퀀스 최대 길이에서 시작토큰과 종료토큰을 제외하고 나머지 데이터를 이용해 정확도를 계산 \n",
    "def accuracy(y_true, y_pred):\n",
    "    # Label (Decoder Input) Size : Batch size x Max length - 1 \n",
    "    y_true1 = tf.reshape(y_true, shape=(-1, 40 - 1 ))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true1, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00fafa6f-20f5-4ccd-ac93-82d77f8fc277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적용 할 Learning Ratet를 각 Node에 따라 가변으로 (동적으로) 조정 \n",
    "learning_rate = transformer_chatbot.CustomSchedule(d_model)\n",
    "# 최적화 함수 설정 / 각 노드 별로 학습율도 조절하면서, 모멘텀 벡터도 계산 \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03b32c1a-89a0-4599-aaae-adc664f24a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=transformer_chatbot.loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8762525-788e-4593-b049-c4c7b8e44882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 392ms/step - accuracy: 0.0179 - loss: 1.4186\n",
      "Epoch 2/20\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 382ms/step - accuracy: 0.0485 - loss: 1.1541\n",
      "Epoch 3/20\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 408ms/step - accuracy: 0.0495 - loss: 0.9602\n",
      "Epoch 4/20\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 406ms/step - accuracy: 0.0518 - loss: 0.8811\n",
      "Epoch 5/20\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 395ms/step - accuracy: 0.0549 - loss: 0.8257\n",
      "Epoch 6/20\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 396ms/step - accuracy: 0.0576 - loss: 0.7744\n",
      "Epoch 7/20\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 397ms/step - accuracy: 0.0613 - loss: 0.7201\n",
      "Epoch 8/20\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 398ms/step - accuracy: 0.0657 - loss: 0.6657\n",
      "Epoch 9/20\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 388ms/step - accuracy: 0.0720 - loss: 0.6095\n",
      "Epoch 10/20\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 385ms/step - accuracy: 0.0801 - loss: 0.5431\n",
      "Epoch 11/20\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 397ms/step - accuracy: 0.0878 - loss: 0.4791\n",
      "Epoch 12/20\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 401ms/step - accuracy: 0.0964 - loss: 0.4126\n",
      "Epoch 13/20\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 397ms/step - accuracy: 0.1056 - loss: 0.3460\n",
      "Epoch 14/20\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 395ms/step - accuracy: 0.1141 - loss: 0.2850\n",
      "Epoch 15/20\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 386ms/step - accuracy: 0.1224 - loss: 0.2293\n",
      "Epoch 16/20\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 401ms/step - accuracy: 0.1303 - loss: 0.1822\n",
      "Epoch 17/20\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 399ms/step - accuracy: 0.1368 - loss: 0.1431\n",
      "Epoch 18/20\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 385ms/step - accuracy: 0.1413 - loss: 0.1160\n",
      "Epoch 19/20\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 391ms/step - accuracy: 0.1456 - loss: 0.0904\n",
      "Epoch 20/20\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 396ms/step - accuracy: 0.1492 - loss: 0.0712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15185cd0050>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset3, epochs=20) # 100회이상 학습시 적절한 성능이 나옴 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd98bec2-ef7d-417c-bfde-78edffba6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력값을 처리하는 함수 \n",
    "def preprocess_sentence(sentence):\n",
    "    # 입력 문장에 대한 특수 기호 처리\n",
    "    sent1 = re.sub(r\"([?.!,])\", r\" \\1 \", sentence).strip()\n",
    "    return sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40c624cd-ee39-418b-8930-b91a854bfc22",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (202853579.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[39], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    sent2 = tf.expand_dims( START_TOKEN + token_model.encode(snet1) _ END_TOKEN)\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "def evaluate(sentence) :\n",
    "    # 입력문장에 대한 전처리\n",
    "    sent1 = preprocess_sentence(sentence)\n",
    "    # 입력문장에 시작토큰과 종료 토큰을 추가\n",
    "    sent2 = tf.expand_dims( START_TOKEN + token_model.encode(snet1) _ END_TOKEN, axis=0)\n",
    "    output = tf.expand_dims( START_TOKEN , 0)\n",
    "\n",
    "    # 디코더에 의한 예측값 시작\n",
    "    for i in range(40) :\n",
    "        predictions = model(input= [sent2, output], trainig = False)\n",
    "\n",
    "        # 모델이 출력에서 마지막 단어를 선택해 이를 바탕으로 다음 단어를 순차적으로 예측\n",
    "        prediconts = predictions[:, -1:, :]\n",
    "        # 예측된 단어를 변수로 선언\n",
    "        pred_id = tf.cast(tf.argmax(prediconts, axis=-1), tf.int32)\n",
    "        # 만약 종료 토큰이 등장하면 예측을 중단\n",
    "        if tf.equal(pred_id, END_TOKEN[0]) :\n",
    "            break\n",
    "        # 현재 시점의 예측단어를 Output에 연결\n",
    "        output = tf.concat([output, pred_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43e7bdc-0e9e-4da8-bb81-90ff5672a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자로 Decoding\n",
    "def predict(sent1) :\n",
    "    pred1 = evaluate(sent1)\n",
    "    pred_sent = token_model.decode( [x for x in pred1 if x < token_model.vocab_size] )\n",
    "    print(\"입력문장 : \" , sent1)\n",
    "    print(\"답변문장 : \", pred_sent)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610fe643-b021-43df-9993-16f966d94ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
