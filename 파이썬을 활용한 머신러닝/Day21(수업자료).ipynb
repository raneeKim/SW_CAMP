{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05182ed7-de1c-4bcb-b42f-c51f937d5123",
   "metadata": {},
   "source": [
    "# Sequence to Sequence \n",
    "\n",
    "- 자연어 처리에서 기존의 일반적인 판별 모델(NLU)은 문장 내 각 단어들을 각 Node가 처리해 하나의 결과를 도출\n",
    "- Sequence to Sequence는 입력된 Sequence를 이용해 하나의 나열된 Sequence를 출력하는 구조를 만들어 학습을 수행\n",
    "- Seq2Seq를 이용하면, 자연어 생성 모델 구성이 가능 (기계번역 / 챗봇 / 문장요약 / STT ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a328265-9b2e-43fd-b6ba-4e65725b5803",
   "metadata": {},
   "source": [
    "![image1](https://wikidocs.net/images/page/24996/%EC%9D%B8%EC%BD%94%EB%8D%94%EB%94%94%EC%BD%94%EB%8D%94%EB%AA%A8%EB%8D%B8.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ee1474-56a0-43f6-8537-3827e31980ed",
   "metadata": {},
   "source": [
    "- Seq2Seq는 LSTM기반의 신경망 알고리즘 구조로 크게 2가지 Part\n",
    "    - 1. Encoder : Input Sequence를 받아서 문장을 특정 Vector로 변환\n",
    "    - 2. Decoder : 새로운 Input Sequence(번역될 문장)를 받아서 Encoder에서 처리된 Contect Vector를 이용해 Output Sequence 생성\n",
    "    - Context : Encoder에 의해 변환된 Vector\n",
    "    - Encoder의 Input과 Decoder의 Input을 Context Vector로 잘 매칭(처리)하여 적절한 Output 도출\n",
    "\n",
    "- 작동 순서\n",
    "  - 1. 입력 문장을 토큰화를 통해 단어(또는 형태소 단위로 처리)\n",
    "  - 2. 단어 토큰을 각각의 RNN계열의 Node에 입력\n",
    "  - 3. Encoder 마지막에 계산된 Node의 정보를 Context Vector로 Decoder에 전달\n",
    "  - 4. Decoder는 앞서 입력받은 Context Vector를 첫 Node에 받아서 처리\n",
    "  - 5. Decoder에 문장의 첫번째를 알리는 초기 토큰 (Start of Sequence, SOS)을 첫 Node에 입력\n",
    "  - 6. 이후, Decoder에서 문장을 이어받아 처리하면서, 다음 등장할 확률이 높은 단어를 예측해 Output\n",
    "  - 7. EOS (End of Sequence)값을 Input Sequence 끝에 넣어, 문장의 마지막을 학습\n",
    "  - 8. Output 값이 생성 될 때, EOS 토큰이 등장하거나 사용자가 지정한 문장의 최대 길이만큼 Output값을 출력 후 생성을 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac26207c-c3b5-44af-891b-0ff74b2506bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input \n",
    "from keras.layers import LSTM \n",
    "from keras.layers import Dense\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94953821-9e7b-4658-97c9-0adbf2d3e044",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'kor.txt'\n",
    "with open(data_path, encoding='UTF-8') as file :\n",
    "    lines = file.read().split('\\n') # 데이터 파일을 불러올 때, 띄어쓰기(\\n)를 기준으로 구분 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea7e431e-4b36-4c16-8eab-1a717c4e173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder와 Decoder에 입력 할 데이터의 구조를 생성\n",
    "# 각 문장을 넣을 리스트 구조를 선언 \n",
    "input_text_list  = [ ] # 영어 문장을 리스트로 선언 \n",
    "target_text_list = [ ] # 한국어 문장을 리스트로 선언 \n",
    "input_characters  = set() # 영어 단어 리스트 (중복 단어가 제거된 단어 사전)\n",
    "target_characters = set() # 한국어 단어 리스트 ( 중복 단어가 제거된 단어 사전) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "799cc91e-af7a-4cc9-a28b-1a7f56745cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3564c05a-b325-4978-84b2-53079cd2d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(lines) # 처리할 문장의 수를 변수로 선언 \n",
    "\n",
    "for line in lines[:num_samples-1]:\n",
    "    # 문장 처리 단계에서 오류 방지를 위한 Try / Except 구문을 활용 \n",
    "    try: \n",
    "        # Tab Key(\\t)를 기준으로 각 문장을 3개의 토큰으로 분할 \n",
    "        input_text, target_text, _ = line.split('\\t') # 3개로 분할 된 각 토큰을 변수로 선언 \n",
    "        # Decoder에 들어갈 Input Seq (Target Text)에 SOS 토큰과 EOS 토큰을 부착 \n",
    "        # Tab Key (SOS)로 Enter Key(EOS) 로 지정하여 부착 \n",
    "        target_text = '\\t' + target_text + '\\n' \n",
    "        # 각각 나눠진 Text를 각 리스트에 추가 \n",
    "        input_text_list.append(input_text)\n",
    "        target_text_list.append(target_text)\n",
    "    except ValueError: # 문장 처리 중 오류가 발생 했을 시 \n",
    "        print('Skip Line ', line) # 어떤 문장이 생략됐는지 확인 \n",
    "        continue # 이어서 진행 \n",
    "    # 분할 된 모든 문장에서 고유 단어만 뽑아 리스트로 선언 (사전 만들기 위한 작업)\n",
    "    for char in input_text:\n",
    "        input_characters.add(char) # input_text의 단어를 가져와 input_characters에 추가\n",
    "    for char in target_text:\n",
    "        target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9977391d-d20f-4b2b-b2f3-88e05744da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 사전을 만들기 위한 리스트 변환 \n",
    "input_char_list = sorted(list(input_characters)) # 영어 문자 토큰을 리스트로 변환후 정렬 \n",
    "target_char_list = sorted(list(target_characters)) # 한국어 문자 토큰을 리스트로 변환후 정렬 \n",
    "\n",
    "# 전체 단어 고유 수 만큼 숫자를 부여하여, 문자 -> 숫자 사전을 구축 \n",
    "num_encoder_token = len(input_char_list) # encoder에 들어갈 영어 문자 수 \n",
    "num_decoder_token = len(target_char_list) # decoder 들어갈 한국어 문자 수 \n",
    "# 영어 문자를 숫자로 변환시킬 Dictionary 구성 \n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_char_list)])\n",
    "# 한국어 문자를 숫자로 변환시킬 Dictionary 구성 \n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_char_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4927dd7b-c121-4419-b97e-bd9ce502a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 문장들 중 최대 길이를 갖는 문장을 확인 (Padding을 수행 하기 위한 목적)  \n",
    "max_encoder_seq_length = max([len(x) for x in input_text_list])\n",
    "max_decoder_seq_length = max([len(x) for x in target_text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20d9f000-e678-40d2-98f0-fe38a5adb24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장을 변환 된 정수로 집어넣을 Matrix를 구성 \n",
    "encoder_input_data = np.zeros( \n",
    "    (len(input_text_list), max_encoder_seq_length,  num_encoder_token), dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(target_text_list), max_decoder_seq_length, num_decoder_token), dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(target_text_list), max_decoder_seq_length, num_decoder_token), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "389b1129-5a03-4f67-886a-867159e5ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding \n",
    "# 구성된 Matrix에 각 단어에 매칭되는 숫자를 입력 \n",
    "# 영어 문장과 한국어 문장을 각각 가져와 동시에 숫자를 Matrix에 입력 \n",
    "for i, (input_text, target_text) in enumerate(zip(input_text_list, target_text_list)):\n",
    "    # 영어 문장에 해당하는 단어를 숫자로 변환하여 Matrix에 추가 \n",
    "    for t , char in enumerate(input_text):\n",
    "       encoder_input_data[i ,t ,input_token_index[char]] = 1\n",
    "    # 한국어 문장에 해당하는 단어를 숫자로 변환하여 Matrix에 추가 \n",
    "    for t , char in enumerate(target_text):\n",
    "        decoder_input_data[i ,t ,target_token_index[char]] = 1 \n",
    "        if t > 0 :\n",
    "            decoder_target_data[i , t-1 ,target_token_index[char]] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9ce571a-038a-4b67-a21d-b7a7d75c8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 시간과 컴퓨터 성능 문제를 고려해 데이터를 간소화 하여 학습을 진행 \n",
    "batch_size = 32 # 훈련 데이터 셋 배치 크기 (64 이상)\n",
    "epochs   = 20 # 훈련 에폭 수 (500회 이상) \n",
    "node_num = 64 # 각 Layer에 들어갈 Node 수 (1024개 이상) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "288fc4ac-5461-4b58-8740-e14d5b3b8f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 구성 \n",
    "# 영어 문장이 들어가 LSTM 모델에 의해 학습이 수행 \n",
    "encoder_input = Input(shape=(None, num_encoder_token))\n",
    "encoder = LSTM(node_num, return_state=True)\n",
    "# state_h : 마지막 Layer의 State / state_c 마지막 Cell State \n",
    "encoder_output, state_h, state_c = encoder(encoder_input)\n",
    "# 인코더가 입력 시퀀스를 처리한 후, 얻은 정보를  encoder_state에 선언\n",
    "# 디코더가 이를 기반으로 출력 시퀀스를 생성 (Context Vector) \n",
    "encoder_state = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df53eca7-4e17-4153-84ac-a2de649f33db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 구성\n",
    "decoder_input = Input(shape=(None, num_decoder_token))\n",
    "# return_sequences=True : 각 단계에서 Seq의 전체 출력을 계산하도록 설정 \n",
    "decoder_lstm  = LSTM(node_num, return_sequences=True, return_state=True)\n",
    "# 앞서 처리된 Encoder의 정보를 불러와 Decoder의 초기 상태로 사용 \n",
    "decoder_output,  _ , _  = decoder_lstm(decoder_input, initial_state=encoder_state )\n",
    "# Softmax의 확률 분포 값을 이용하여 출력값의 확률 값을 계산 \n",
    "decoder_dense  = Dense(num_decoder_token, activation='softmax')\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "# Seq2Seq 모델을 정의 \n",
    "# 두개의 Input이 들어가, 하나의 Output 나오는 구조\n",
    "model = Model( [encoder_input, decoder_input] , decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1c8e4959-7ac1-4f25-bc21-b093c96d3e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 561ms/step - loss: 0.3195 - val_loss: 0.3291\n",
      "Epoch 2/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 401ms/step - loss: 0.3023 - val_loss: 0.3185\n",
      "Epoch 3/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 417ms/step - loss: 0.2894 - val_loss: 0.3017\n",
      "Epoch 4/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 412ms/step - loss: 0.2747 - val_loss: 0.2915\n",
      "Epoch 5/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 342ms/step - loss: 0.2672 - val_loss: 0.2858\n",
      "Epoch 6/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 387ms/step - loss: 0.2628 - val_loss: 0.2837\n",
      "Epoch 7/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 412ms/step - loss: 0.2607 - val_loss: 0.2820\n",
      "Epoch 8/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 403ms/step - loss: 0.2591 - val_loss: 0.2802\n",
      "Epoch 9/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 387ms/step - loss: 0.2578 - val_loss: 0.2795\n",
      "Epoch 10/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 388ms/step - loss: 0.2577 - val_loss: 0.2792\n",
      "Epoch 11/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 375ms/step - loss: 0.2575 - val_loss: 0.2781\n",
      "Epoch 12/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 365ms/step - loss: 0.2567 - val_loss: 0.2772\n",
      "Epoch 13/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 343ms/step - loss: 0.2532 - val_loss: 0.2762\n",
      "Epoch 14/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 326ms/step - loss: 0.2550 - val_loss: 0.2752\n",
      "Epoch 15/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 378ms/step - loss: 0.2524 - val_loss: 0.2731\n",
      "Epoch 16/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 363ms/step - loss: 0.2708 - val_loss: 0.2837\n",
      "Epoch 17/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 392ms/step - loss: 0.2582 - val_loss: 0.2766\n",
      "Epoch 18/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 346ms/step - loss: 0.2521 - val_loss: 0.2733\n",
      "Epoch 19/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 399ms/step - loss: 0.2519 - val_loss: 0.2717\n",
      "Epoch 20/20\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 347ms/step - loss: 0.2502 - val_loss: 0.2711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a88f4b4e10>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 실시 \n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.fit( [encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "           batch_size=batch_size, epochs=epochs, validation_split=0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c30e9b73-a1ea-4057-8615-cd20980e2c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 이후(NLU) 추론 및 생성(NLG)모델을 구성 \n",
    "# 학습 과정에서 구성한 인코더의 입력과 인코더의 상태를 기반으로 새로운 모델을 생성 \n",
    "encoder_model = Model(encoder_input, encoder_state)\n",
    "# 디코더의 LSTM 레이어에 전달될 초기 상태를 정의하는 입력 Layer를 구성 \n",
    "decoder_state_input_h = Input(shape=(node_num, )) # Layer State를 초기화 \n",
    "decoder_state_input_c = Input(shape=(node_num, )) # Cell State를 초기화 \n",
    "# 초기 Decoder의 Input Node를 구성 \n",
    "decoder_state_input = [decoder_state_input_h, decoder_state_input_c]\n",
    "# 구성된 Input Node를 이용해, Decoder의 Layer를 구성 \n",
    "decoder_output, state_h, state_c = decoder_lstm(\n",
    "                                decoder_input, initial_state= decoder_state_input)\n",
    "# 다음 Step으로 전달 될 상태를 지정 \n",
    "decoder_state = [state_h, state_c]\n",
    "# 확률 분포를 계산하는 Decoder Output에 전달 하여 Update \n",
    "decoder_ouput_proba = decoder_dense(decoder_output)\n",
    "# 다음 단계의 토큰 (단어, 문자)를 예측하고 다음 문장을 생성하기 위한 생태를 생성 \n",
    "decoder_model= Model([decoder_input]+ decoder_state_input, \n",
    "                     [decoder_output]+decoder_state)\n",
    "# Decoder가 계산 결과 이용해, 역매핑(숫자->문자)변환 작업을 수행 \n",
    "reverse_input_char_index = dict( (i, char) for char, i in input_token_index.items() )\n",
    "reverse_target_char_index = dict( (i, char) for char, i in target_token_index.items() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c22a1b57-774a-4299-8443-4fc2c1c3091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 문장이 들어오면, 해당 문장을 번역하는 함수를 구성 \n",
    "def decode_sequence(input_seq):\n",
    "    # 입력 받은 Sequence를 이용해, Encoder에 넣어 Context Vector를 예측 \n",
    "    state_value = encoder_model.predict(input_seq, verbose=0)\n",
    "    \n",
    "    # Decoder의 초기 Sequence를 결정 \n",
    "    target_Seq = np.zeros((1,1,num_decoder_token))\n",
    "    # 시작토큰 (SOS)의 위치를 1로 설정해서, Decoder가 예측을 시작할 부분을 지정 \n",
    "    target_Seq[0,0, target_token_index['\\t']] = 1 \n",
    "\n",
    "    # 디코딩이 종료가 될 시점의 변수를 지정 \n",
    "    # 문장 끝에 EOS가 오거나 또는 사용자가 지정한 문장 최대 길이에 도달하면 생성이 중지 \n",
    "    stop_condition = True # 종료조건이 오면, 해당 변수를 False \n",
    "    decoder_sentence = '' # 생성될 문자열이 들어갈 변수 \n",
    "    while stop_condition :\n",
    "        # 디코더의 Layer 정보를 이용해서 Ouput Token을 출력 \n",
    "        output_token, h, c = decoder_model.predict(\n",
    "                                        [target_Seq] + state_value, verbose=0)\n",
    "        # 가장 확률이 높게 계산된 숫자(단어의 Index)를 변수로 선언 \n",
    "        sampled_token_index = np.argmax(output_token[0, -1 , : ])\n",
    "        # 해당 Index를 문자로 변환 \n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        # 변환된 문자를 decoder_sentence에 추가 \n",
    "        decoder_sentence += sampled_char\n",
    "\n",
    "        # EOS 나 최대 문장 길이에 도달할 경우, Stop Condition을 False 바꿔 반복을 종료 \n",
    "        if (sampled_char=='\\n' or len(decoder_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = False \n",
    "        # 종료가 되지 않을 시, 다음 단어(문자)를 예측하기 위해 초기화 \n",
    "        target_Seq = np.zeros((1,1,num_decoder_token))\n",
    "        target_Seq[0,0, sampled_token_index] = 1 \n",
    "        state_value = [h,c]\n",
    "    # 종료조건에 의해 반복문이 멈추면 생성된 문장을 출력 \n",
    "    return decoder_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d219e3ba-6d61-49cd-9547-3fc80471a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 문장을 직접 입력해 번역 실행 \n",
    "def preprocess_input_sequence(setence):\n",
    "    input_seq = np.zeros((1, max_encoder_seq_length, num_encoder_token), dtype='float32')\n",
    "    for i, char in enumerate(setence):\n",
    "        # 입력 문장의 각 문자에 대해 One Hot Encoding 수행 \n",
    "        input_seq[0, i, input_token_index[char]] = 1 \n",
    "    return input_seq # Text to Sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e59cdfaf-8d9d-43a6-b1b0-da6839d22698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환된 Sequence를 모델에 넣어 번역 문자 생성 \n",
    "def translate(input_text):\n",
    "    # 입력문장 전처리\n",
    "    input_seq = preprocess_input_sequence(input_text)\n",
    "    # 번역을 수행 \n",
    "    decode_sentence = decode_sequence(input_seq)\n",
    "    return decode_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ad4c0236-62c0-472e-aa40-ef1a72017930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'갔갔갔갔갔aaaaBB0000    ,  ,,    \"\",   ,  ,  \",   /\",   , \"    \",  \" \"  /\",    \"\"   ,  \",    \",  \" ,\"\"    \"  \" \" \"/,    /\"      \"/  \"  ,/\"\"   \" ,,     \"\"   \"  \",   /  \"\"   /  //,   \"\"\"\"    \" \"\"   , \",  ,  \"  /,  ,,  \"     \"\"       /,  \"  \" \"\"   / \" \"\"\"\"   ,  \" ,\"  \"/  \"   /     /,,    ,\"\"    ,  \"\"  ,,  '"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 구성된 함수에 입력문장을 넣기 \n",
    "input_sentence = \"Show me the money!\"\n",
    "translate(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "98ab58ef-4485-4683-90b6-468d68332b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'갔갔갔갔갔aaaaBB0000    ,  ,,    \"\",   ,  ,  \",   /\",   , \"    \",  \" \"  /\",    \"\"   ,  \",    \",  \" ,\"\"    \"  \" \" \"/,    /\"      \"/  \"  ,/\"\"   \" ,,     \"\"   \"  \",   /  \"\"   /  //,   \"\"\"\"    \" \"\"   , \",  ,  \"  /,  ,,  \"     \"\"       /,  \"  \" \"\"   / \" \"\"\"\"   ,  \" ,\"  \"/  \"   /     /,,    ,\"\"    ,  \"\"  ,,  '"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = \"This is pen.\"\n",
    "translate(input_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
