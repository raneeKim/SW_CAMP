{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6c4822-0eb6-49f8-831b-a322b2a59376",
   "metadata": {},
   "source": [
    "# Sequece to Sequence\n",
    "\n",
    "- 자연어 처리에서 기존의 일반적인 판별 모델(NLU)은 문장 내 각 단어들을 각 Node가 처리해 하나의 결과를 도출\n",
    "- Sequence to Sequence는 입력된 Sequence를 이용해 하나의 나열된 Sequence를 출력하는 구조를 만들어 학습을 수행\n",
    "- Seq2Seq를 이용하면, 자연어 생성 모델 구성이 가능 (기계번역 / 챗봇 / 문장요약 / STT..)\n",
    "  사진추추추가가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c99c745-0491-42d2-a4f3-c7259b1b98a7",
   "metadata": {},
   "source": [
    "- Seq2Seq는 LSTM기반의 신경망 알고리즘 구조로 크게 2가지 Part\n",
    "  1. Encoder : Input Sequence를 받아서 문장을 특정 Vector로 변환\n",
    "  2. Decoder : 새로운 Input Sequence(번역된 문장)을 받아서 Encoder에서 처리된 Contect Vector를 이용해 Output Sequece 생성\n",
    "     - Context : Encoder에 의해 변환된 Vector\n",
    "     - Encoder의 Input과 Decoder의 Input을 Context Vector로 잘 매칭(처리)하여 적절한 Output 도출\n",
    "\n",
    "- 작동 순서\n",
    "  1. 입력 문장을 토큰화를 통해 단어(또는 형태소 단위로 처리)\n",
    "  2. 단어 토큰을 각각 RNN계열의 Node에 입력\n",
    "  3. Encoder 마지막에 계산된 Node의 정보를 Context Vector로 Decoder에 전달\n",
    "  4. Decoder는 앞서 입력받은 Context Vector를 첫 Node에 받아서 처리\n",
    "  5. Decoder에 문장의 첫번째를 알리는 초기 토큰 (Start of Sequence, SOS)을 첫 Node에 입력\n",
    "  6. 이후, Decoder에서 문장을 이어받아 처리하면서, 다음 등장할 확률이 높은 단어를 예측해 Output\n",
    "  7. EOS (End of Sequence)값을 Input Sequence 끝에 넣어, 문자으이 마지막을 학습\n",
    "  8. Output값이 생성 될 때, EOS 토큰이 등장하거나 사용자가 지정한 문장의 최대 길이 만큼 Output 값을 출력 후 생성을 종료\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aba630d3-c7e1-4f3e-a427-899cc94c9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639efa23-c677-4300-aa7e-8f6fe27343ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path =  r'C:\\Users\\UserK\\Desktop\\Ranee\\data\\ML\\kor.txt' \n",
    "with open(data_path, encoding = \"UTF-8\") as file :\n",
    "    lines = file.read().split('\\n') # 데이터 파일을 불러올 때, 띄워쓰기를 기준으로 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b6437c0-8e85-4265-8325-a5c6a1f021d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder와 Decoder에 입력할때 데이터의 구조를 생성\n",
    "# 각 문장을 넣을 리스트 구조를 선언\n",
    "input_text_list = [] # 영어 문장을 리스트로 선언\n",
    "target_text_list = [] # 한국어 문장을 리스트로 선언\n",
    "input_characters = set() # 영어 단어 리스트 (중복단어 제거된 단어 사전)\n",
    "target_characters = set() # 한국어 단어 리스트 (중복 단어가 제거된 단어 사전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99b2c872-5891-4db4-aca1-ab915f0cc50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(lines) # 처리할 문장의 수를 변수로 선언\n",
    "for line in lines[0:num_samples-2]:\n",
    "    # 문장 처리 단계에서 오류 방지를 위한 Try / Except 구문을 활용\n",
    "    try:\n",
    "        # tab key를 기준으로 각 문장을 3개의 토큰으로 분할\n",
    "        input_text, target_text,_ = line.split('\\t')\n",
    "        # Decoder에 들어갈 Inpit Seq(Target Text)에 SOS토큰과 EOS토큰을 부착\n",
    "        target_text = '\\t' + target_text + '\\n' # tab key(SOS)로 Enter Key를 (EOS)로 지정하여 부착\n",
    "        # 각각 나눠진 Text를 각 리스트에 추가\n",
    "        input_text_list.append(input_text)\n",
    "        target_text_list.append(target_text)\n",
    "    except ValueError: # 문장 처리 중 오류가 발생 했을 시\n",
    "        print('Skip Line', line) # 어떤 문장이 생략됬는지 확인\n",
    "        continue # 이어서 진행\n",
    "    # 분할 된 모든 문장에서 고유 단어만 뽑아 리스트로 선언(사전 만들기 위한 작업 ) \n",
    "    for char in input_text:\n",
    "        input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ed1dddb-339c-48c5-9d70-85483d746f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 사전을 만들기 위한 리스트 변환\n",
    "input_char_list = sorted(list(input_characters)) # 영어 문자 토큰을 리스트로 변환 후 정렬\n",
    "target_char_list = sorted(list(target_characters)) # 한국어 문자 토큰을 리스트로 변환 후 정렬\n",
    "\n",
    "# 전체 단어 고유 수 만큼 부여하여. 문자 -> 숫자 사전을 구축\n",
    "num_encoder_token = len(input_char_list) # encoder에 들어갈 영어 문자 수 \n",
    "num_decoder_token = len(target_char_list) # decoder에 들어갈 한국어 문자 수\n",
    "\n",
    "# 영어 문자를 숫자로 변환시킬 Dictionary 구성\n",
    "input_token_index = dict([(char,i) for i , char in enumerate(input_char_list)])\n",
    "# 한국어 문자를 숫자로 변환시킬 Dictionary 구성\n",
    "target_token_index = dict([(char,i) for i , char in enumerate(target_char_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a7bd651-ba9d-4bac-ab06-4510da85d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 문장들 중 최대 길이를 확인\n",
    "max_encoder_seq_length = max([len(x) for x in input_text_list])\n",
    "max_decoder_seq_length = max([len(x) for x in target_text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c58641b0-3bd4-4bd1-94b1-ddc08eae8f76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# 각 문장을 변환 된 정수로 집어넣어 Matrix를 구성\n",
    "encoder_input_data = np.zeros((len(input_text_list), max_encoder_seq_length, num_encoder_token), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(target_text_list), max_decoder_seq_length, num_decoder_token), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(target_text_list), max_decoder_seq_length, num_decoder_token), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38ddff2e-ea13-44c7-8472-40a44e2ec8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "# 구성된 Matrix에 각 단어에 매칭되는 숫자를 입력\n",
    "# 영어 문장과 한국어 문장을 각각 가져와 동시에 숫자를 Matrix에 입력\n",
    "for i, (input_text, target_text) in enumerate(zip(input_text_list, target_text_list)) :\n",
    "    # 영어 문장에 해당하는 단어를 숫자로 변환하여 Maxtrix에 추가\n",
    "    for t, char in enumerate(input_text) :\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1\n",
    "    # 한국어 문장에 해당하는 단어를 숫자로 변환하여 Maxtrix에 추가\n",
    "    for t, char in enumerate(target_text) :\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1\n",
    "        if t > 0 :\n",
    "            decoder_target_data[i, t-1, target_token_index[char]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33252395-1ef0-44ea-8e87-795f8216d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 시간과 컴퓨터 성능 문제를 고려해 데이터를 간소화 하여 학습을 진행\n",
    "batch_size = 32 # 훈련 데이터 셋 배치 크기(64이상)\n",
    "epochs = 20 # 훈련 에포크 수(500회 이상)\n",
    "node_num = 64 # 각 Layer에 들어갈 Node수(1024개 이상)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d640c610-a653-4619-bc51-2f79e6b0ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 구성\n",
    "# 영어 문장이 들어가 LSTM 모델에 의해 학습이 수행\n",
    "\n",
    "encoder_input = Input(shape=(None, num_encoder_token))\n",
    "encoder = LSTM(node_num, return_state = True)\n",
    "\n",
    "# state_h : 마지막 Layer의 State / state_c 마지막 Cell State\n",
    "encoder_ouput , state_h , state_c = encoder(encoder_input)\n",
    "\n",
    "# 인코더가 입력 시퀀스를 처리한 후, 얻은 정보를 encoder_state에 선언\n",
    "# 디코더가 이를 기반으로 출력 시퀀스를 생성 (Context Vector)\n",
    "encoder_state = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e46e4e0f-61bb-4a76-a6b9-832da9a087fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 구성\n",
    "decoder_input = Input(shape=(None, num_decoder_token))\n",
    "\n",
    "# return_sequence=True  : 각 단계에서 Seq의 전체 출력을 계산하도록 설정\n",
    "decoder_lstm = LSTM(node_num, return_sequences=True, return_state=True)\n",
    "\n",
    "# 앞서 처리된 Encoder의 정보를 불러와 Decoder의 초기 상태로 사용\n",
    "decoder_output, _, _ = decoder_lstm(decoder_input, initial_state = encoder_state)\n",
    "\n",
    "# Softmax의 확률 분포 값을 이용하여 출력값의 확률 값을 계산\n",
    "decoder_dense = Dense(num_decoder_token, activation='softmax')\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "# Seq2Seq 모델 정의\n",
    "# 두개의 Input이 들어가, 하나의 Output 나오는 구조\n",
    "model = Model( [encoder_input, decoder_input] , decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "29ff77b3-c6b0-4cff-9d66-145ffbbd8c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 102ms/step - loss: 0.6232 - val_loss: 1.0328\n",
      "Epoch 2/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 95ms/step - loss: 0.6066 - val_loss: 1.0244\n",
      "Epoch 3/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 97ms/step - loss: 0.6024 - val_loss: 1.0198\n",
      "Epoch 4/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 95ms/step - loss: 0.5951 - val_loss: 1.0085\n",
      "Epoch 5/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 97ms/step - loss: 0.5886 - val_loss: 0.9907\n",
      "Epoch 6/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 104ms/step - loss: 0.5780 - val_loss: 0.9818\n",
      "Epoch 7/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 99ms/step - loss: 0.5705 - val_loss: 0.9717\n",
      "Epoch 8/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 99ms/step - loss: 0.5726 - val_loss: 0.9700\n",
      "Epoch 9/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 99ms/step - loss: 0.5672 - val_loss: 0.9675\n",
      "Epoch 10/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 96ms/step - loss: 0.5633 - val_loss: 0.9625\n",
      "Epoch 11/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 95ms/step - loss: 0.5629 - val_loss: 0.9613\n",
      "Epoch 12/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 96ms/step - loss: 0.5589 - val_loss: 0.9498\n",
      "Epoch 13/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 95ms/step - loss: 0.5595 - val_loss: 0.9491\n",
      "Epoch 14/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 95ms/step - loss: 0.5561 - val_loss: 0.9417\n",
      "Epoch 15/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 95ms/step - loss: 0.5466 - val_loss: 0.9455\n",
      "Epoch 16/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 103ms/step - loss: 0.5502 - val_loss: 0.9410\n",
      "Epoch 17/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 104ms/step - loss: 0.5457 - val_loss: 0.9344\n",
      "Epoch 18/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 107ms/step - loss: 0.5852 - val_loss: 0.9562\n",
      "Epoch 19/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 101ms/step - loss: 0.5604 - val_loss: 0.9427\n",
      "Epoch 20/20\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 98ms/step - loss: 0.5531 - val_loss: 0.9434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d83a17c850>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 실시\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.fit( [encoder_input_data, decoder_input_data] , decoder_target_data,\n",
    "          batch_size = batch_size, epochs=epochs, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6af88b47-4ba5-44d3-a0e7-bbfb69b95894",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer 'dense_2' expected 1 input(s). Received 2 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m [state_h, state_c]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 확률 분포를 계산하는 Decoder Output에 전달 하여 Update\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m decoder_output_proba \u001b[38;5;241m=\u001b[39m decoder_dense(decoder_output)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 다음 단계의 토큰(단어,문자)를 예측하고 다음 문장을 생성하기 위한 생태를 생성\u001b[39;00m\n\u001b[0;32m     22\u001b[0m decoder_model \u001b[38;5;241m=\u001b[39m Model([decoder_input] \u001b[38;5;241m+\u001b[39m decoder_state_input,\n\u001b[0;32m     23\u001b[0m                       [decoder_output]\u001b[38;5;241m+\u001b[39mdecoder_state_)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:156\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    154\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mflatten(inputs)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_spec) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs):\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input(s). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# Having a shape/dtype is the only commonality of the various\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# tensor-like objects that may be passed. The most common kind of\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# invalid type we are guarding for is a Layer instance (Functional API),\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# which does not have a `shape` attribute.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: Layer 'dense_2' expected 1 input(s). Received 2 instead."
     ]
    }
   ],
   "source": [
    "# 학습 이후(NLU) 추론 및 생성(NLG) 모델을 구성\n",
    "# 학습 과정에서 구성한 인코더의 입력과 인코더의 상태를 기반으로 새로운 모델을 생성\n",
    "encoder_model = Model(encoder_input, encoder_state)\n",
    "\n",
    "# 디코더의 LSTM 레이어에 전달된 초기 상태를 정의하는 입력 Layer를 구성\n",
    "decoder_state_input_h = Input(shape=(node_num, )) # Layer State를 초기화\n",
    "decoder_state_input_c = Input(shape=(node_num, )) # Cell State를 초기화\n",
    "\n",
    "# 초기 Decoder의 Input Node를 구성\n",
    "decoder_state_input = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 구성된 Input Node를 이용해, Decoder의 Layer를 구성\n",
    "decoder_output, state_h, state_c = decoder_lstm(decoder_input, initial_state= decoder_state_input)\n",
    "\n",
    "# 다음 Step으로 전달 될 상태를 지정\n",
    "decoder_output = [state_h, state_c]\n",
    "\n",
    "# 확률 분포를 계산하는 Decoder Output에 전달 하여 Update\n",
    "decoder_output_proba = decoder_dense(decoder_output)\n",
    "\n",
    "# 다음 단계의 토큰(단어,문자)를 예측하고 다음 문장을 생성하기 위한 생태를 생성\n",
    "decoder_model = Model([decoder_input] + decoder_state_input,\n",
    "                      [decoder_output]+decoder_state_)\n",
    "\n",
    "\n",
    "# Decoder가 계산 결과 이용해, 역매핑 (숫자->문자) 변환 작업으르 수행\n",
    "reverse_input_char_index = dict( (i,char) for char,i in input_token_index.items())\n",
    "reverse_target_char_index = dict( (i,char) for char,i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e31d3d-c57c-46f2-ab18-867e76e012ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 문장이 들어오면, 해당 문장을 번역하는 함수를 구성\n",
    "def decode_sequence(input_seq) :\n",
    "    # 입력 받은 Sequence를 이용해, Encoder에 넣어 Context Vector를 예측\n",
    "    state_value = encoder_model.predict(input_seq, verbose = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff7243-db6f-4b4f-9faa-7fce2aa03542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input_sequence(sentence) :\n",
    "    input_seq = np.zeros((1,max_encoder_seq_length, num_encoder_token), dtype='float32')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
